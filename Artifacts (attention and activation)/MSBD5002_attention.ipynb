{"cells":[{"cell_type":"markdown","metadata":{"id":"9pwyoeIgxXk7"},"source":["### Loading the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["96cb2681d1514570b7533fd0fd8a43d7","63bc0b9194fd45e1a8b8ce3e31f925cd","252b35975fbf4e21978d5147f0940c63","0ad57e7b467f465988d0f9fd45f0bc3e","a83f191f1dec48b89c3387ca769c0415","a5afc4dfba6d487b99e63270fd108255","ae2df56e97bd4c238be039e411542534","e3890e60fc024b4aaaa197ff5ed6fbc0","9d2ffdd63e4e4d778db2b5954de9027b","9e93eb50ee86499582b6a595f37991c7","28569c8894424c6681cf4b6e307c4b89"]},"id":"f6KgX4BxxZP-","outputId":"49a2b28b-74f3-4fbd-fbe6-1484d8391cf0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96cb2681d1514570b7533fd0fd8a43d7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["OpenLLaMA-7b loaded successfully!\n"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","model_id = \"openlm-research/open_llama_7b\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","print(\"OpenLLaMA-7b loaded successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"NcbHCcDsxclz"},"source":["### Loading the dataset and connect to google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2qknJhDx_Yq","outputId":"0f289a2a-05c7-4f41-e065-2818392e856b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["pip install datasets"]},{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/MSBD5002_project/' # modify this line for your drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J4cIQ49zPOxr","outputId":"19d6601d-8165-4add-f48f-55504a568c01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQGqMRCoy41U","outputId":"c6907a68-38d2-442a-ad57-7fa8b3ae0b6c"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 138384/138384 [00:24<00:00, 5673.89it/s]\n"]}],"source":["from datasets import load_dataset\n","\n","# Load the TriviaQA dataset\n","trivia_qa = load_dataset('trivia_qa', 'rc.nocontext')\n","from tqdm import tqdm\n","full_dataset = []\n","for obs in tqdm(trivia_qa['train']):\n","  aliases = []\n","  aliases.extend(obs['answer']['aliases'])\n","  aliases.extend(obs['answer']['normalized_aliases'])\n","  aliases.append(obs['answer']['value'])\n","  aliases.append(obs['answer']['normalized_value'])\n","  full_dataset.append((obs['question'], aliases))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4U8p6CwzFlE"},"outputs":[],"source":["data = full_dataset[6000:9000] # choosing the size of the dataset"]},{"cell_type":"markdown","metadata":{"id":"fzq3wZgC0C6f"},"source":["### Get the attention score (implementation)\n","##### if you want the classifer, you can directly jump to the classifier part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"I9wBZed-zuZt","outputId":"184cb071-7dee-42d6-f075-d2657c84a7a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 1/2 [00:04<00:04,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n","-------------\n","\n","Which American-born Sinclair won the Nobel Prize for Literature in 1930? Sinclair Lewis\n","Which American-born Sinclair won the Nobel Prize for Literature in 1930? Sinclair Lewis.\n","Which American-born Sinclair won the Nobel\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:06<00:00,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Where in England was Dame Judi Dench born?\n","-------------\n","\n","Dame Judi Dench was born in York, England.\n","Q: Where in England was Dame Judi Dench born?\n","Where was Dame Judi Dench\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nsave_data = {\\n    \"attention_scores\": attention_total_list,\\n    \"labels\": label_list\\n}\\ntorch.save(save_data, data_path + f\"attention_data_{(i+2)*1000}.pt\") #store the result to google drive\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["import torch\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/MSBD5002_project/'\n","\n","\n","model.to('cuda')\n","model.eval()\n","attention_total_list = []\n","label_list = []\n","for item in tqdm(data):\n","  prompt = item[0]\n","  answer = item[1]\n","\n","  #initialization\n","  attention_scores = []\n","  qm_a1_scores = []\n","  response = []\n","  inputs_id = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n","  start_pos = inputs_id.shape[-1]\n","  num_layers = len(model.model.layers)\n","  idx = num_layers - 1\n","  prompt_len = len(inputs_id[0])\n","\n","\n","  #getting the predicted token\n","  max_length = prompt_len * 3\n","  bar = range(max_length)\n","\n","  for i in bar:\n","    with torch.no_grad():\n","        outputs = model(inputs_id, output_attentions=True)\n","    next_token = outputs.logits[:, -1, :].argmax(dim=-1)\n","    inputs_id = torch.cat([inputs_id, next_token.unsqueeze(-1)], dim=-1)\n","    response.append(next_token)\n","\n","\n","    # Store the attention weights\n","    last_layer_attn = outputs.attentions[-1].detach().cpu()\n","    attention_scores.append(last_layer_attn)\n","\n","    #print(last_layer_attn.shape) #(1, 32, prompt_len, prompt_len)\n","    # Extract S_L(q_M, a_1)\n","\n","    if i == 0:\n","      qm_idx = prompt_len - 1  # Last token of prompt\n","      a1_idx = prompt_len      # First response token\n","\n","    if i == max_length - 1:\n","      response_list = torch.stack(response).cpu().numpy()\n","      str_response = tokenizer.decode(response_list.flatten(), skip_special_tokens=True) # obtain the string response\n","      label = 0\n","\n","      for alias in answer:    # checking the correctness of the anwser\n","        if alias.lower() in str_response.lower():\n","          label = 1\n","          break\n","      label_list.append(label)\n","\n","\n","  attention_required = attention_scores[1][0, :, a1_idx, qm_idx] # get the required attention score\n","  attention_total_list.append(attention_required)\n","\n","\"\"\"\n","save_data = {\n","    \"attention_scores\": attention_total_list,\n","    \"labels\": label_list\n","}\n","torch.save(save_data, data_path + f\"attention_data_{(i+2)*1000}.pt\") #store the result to google drive\n","\"\"\"\n"]},{"cell_type":"markdown","source":["### Get the attention (Research part - stacked approach)\n","This part is independent to the implementation part"],"metadata":{"id":"38GjwSb9d2Ci"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import pipeline\n","import spacy\n","\n","# Load BERT-based NER model\n","tokenizer_bert = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n","model_bert = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n","ner_pipeline = pipeline(\"ner\", model=model_bert, tokenizer=tokenizer_bert, aggregation_strategy=\"simple\")\n","\n","# Load spaCy for fallback POS and dependency parsing\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JznHc6rPvccu","outputId":"5b3537cb-cf4b-4cf3-c27b-74f7e597f32d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Here is the part for Extractive summarization\n","\"\"\"\n","def extract_keyword(text):\n","    # Step 1: Run BERT-based NER\n","    ner_results = ner_pipeline(text)\n","\n","    # Extract the first entity (PERSON, GPE, ORG, etc.)\n","    for entity in ner_results:\n","        return entity[\"word\"]  # e.g., \"Sinclair\", \"England\", \"Super Bowl XX\"\n","\n","    # Step 2: Fallback to spaCy for non-entity nouns\n","    doc = nlp(text)\n","    for token in doc:\n","        # Prioritize proper nouns (PROPN) or significant nouns (NOUN)\n","        if token.pos_ in [\"PROPN\", \"NOUN\"] and token.dep_ in [\"nsubj\", \"dobj\", \"pobj\", \"ROOT\"]:\n","            # Handle multi-word proper nouns\n","            if token.pos_ == \"PROPN\" and token.head.pos_ == \"PROPN\" and token.head != token:\n","                return f\"{token.text} {token.head.text}\"\n","            return token.text\n","\n","    return \"No keyword found\"\n","\n","# Demo for keyword extraction\n","prompts= [data[i][0] for i in range(1)]\n","for prompt in prompts:\n","    keyword = extract_keyword(prompt)\n","    print(f\"Prompt: {prompt} -> Keyword: {keyword}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxUTgWItjKcl","outputId":"c389eb15-d06f-4796-a21c-26be23e47962"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Mars has two moons. Phobos is one, but what is the other one called? -> Keyword: Mars\n","Prompt: In the 1966 movie The Good, The Bad And The Ugly, Clint Eastwood played the Good\" and Lee van Cleef played \"the Bad\", but who played \"the Ugly\"? -> Keyword: The Good, The Bad And The Ugly\n","Prompt: The Mauretania, launched in 1906, was the largest and fastest ship in the world at that time. What was the name of her sister ship that was launched in the same year, but sunk by a German U-boat in 1915? -> Keyword: Ma\n","Prompt: The Ballearics are made up of three major islands. Majorca and Minorca are two, but what is the other one? -> Keyword: Ball\n","Prompt: Neil Armstrong and Buzz Aldrin walked on the moon in 1969, but who was the third astronaut on Apollo 11, who remained in the orbitter? -> Keyword: Neil Armstrong\n"]}]},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import time\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/MSBD5002_project/' # modify this to your drive\n","\n","\n","model.to('cuda')\n","model.eval()\n","\n","#initialization\n","attention_total_list_a1_all = []\n","attention_total_list_a1_out = []\n","attention_total_list_a1 = []\n","attention_total_list_qm_all = []\n","attention_total_list_qm_out = []\n","label_list = []\n","\n","start_time = time.time()\n","for item in tqdm(data):\n","  prompt = item[0]\n","  answer = item[1]\n","\n","  #initialization\n","  attention_scores = []\n","  qm_a1_scores = []\n","  response = []\n","  inputs_id = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n","  start_pos = inputs_id.shape[-1]\n","  num_layers = len(model.model.layers)\n","  idx = num_layers - 1\n","  prompt_len = len(inputs_id[0])\n","  keyword_id = tokenizer(extract_keyword(prompt), return_tensors=\"pt\").input_ids.to(\"cuda\")[0][1].item()\n","  original_inputs_id = inputs_id.clone()\n","\n","  qm_idx = prompt_len - 1 # finding position of the keyword\n","  for k in range(prompt_len):\n","    if keyword_id == inputs_id[0][k].item():\n","      qm_idx = k\n","\n","\n","  #getting the predicted token\n","  max_length = 3 * prompt_len\n","  bar = range(max_length)\n","  stop_token = 13\n","\n","  for i in bar:\n","    with torch.no_grad():\n","        outputs = model(inputs_id, output_attentions=True)\n","    next_token = outputs.logits[:, -1, :].argmax(dim=-1)\n","    inputs_id = torch.cat([inputs_id, next_token.unsqueeze(-1)], dim=-1)\n","    response.append(next_token)\n","\n","\n","    # Store the attention weights\n","    last_layer_attn = outputs.attentions[-1].detach().cpu()\n","    attention_scores.append(last_layer_attn)\n","\n","    # Break the loop if the stop token is reached\n","    if next_token.item() == stop_token and i > 5:\n","      response_list = torch.stack(response).cpu().numpy()\n","      str_response = tokenizer.decode(response_list.flatten(), skip_special_tokens=True)\n","      break\n","\n","    #print(last_layer_attn.shape) #(1, 32, prompt_len, prompt_len)\n","\n","    if i == max_length - 1:\n","      response_list = torch.stack(response).cpu().numpy()\n","      str_response = tokenizer.decode(response_list.flatten(), skip_special_tokens=True)\n","\n","    \"\"\"\n","      label = 0\n","      for alias in answer:\n","        if alias.lower() in str_response.lower():\n","          label = 1\n","          break\n","      label_list.append(label)\n","      print(prompt)\n","      print('-------------')\n","      print(str_response) #the response from the model\n","\n","    \"\"\"\n","  # Remove the repeated word that appearing in the input prompt for the generated response\n","  words_to_remove = prompt.split()\n","  filtered_output = str_response\n","  for word in words_to_remove:\n","    filtered_output = filtered_output.replace(word, \"\")\n","\n","  a1_idx = prompt_len\n","\n","  # Find the index for the keyword in response\n","  if extract_keyword(filtered_output) != \"No keyword found\":\n","    keyword_id = tokenizer(extract_keyword(filtered_output), return_tensors=\"pt\").input_ids.to(\"cuda\")[0][1].item()\n","    for k in range(len(inputs_id[0]) - 1 - prompt_len):\n","      if keyword_id == inputs_id[0][k + prompt_len].item():\n","        a1_idx = k + prompt_len\n","        break\n","\n","  # Store the artifacts we want to extract\n","  attention_required_a1 = attention_scores[-1][0, :, a1_idx, prompt_len - 1]\n","  attention_required_a1_divall = attention_scores[-1][0, :, a1_idx, prompt_len - 1] / ( i + prompt_len)\n","  attention_required_qm_divall = attention_scores[-1][0, :, -1, qm_idx] / ( i + prompt_len)\n","  if i != 0:\n","    attention_required_qm_divoutput = attention_scores[-1][0, :, -1, qm_idx] / i\n","    attention_required_a1_divoutput = attention_scores[-1][0, :, a1_idx, prompt_len - 1] / i\n","  else:\n","    attention_required_qm_divoutput = attention_scores[-1][0, :, -1, qm_idx]\n","    attention_required_a1_divoutput = attention_scores[-1][0, :, a1_idx, prompt_len - 1]\n","\n","  attention_total_list_a1.append(attention_required_a1)\n","  attention_total_list_a1_all.append(attention_required_a1_divall)\n","  attention_total_list_a1_out.append(attention_required_a1_divoutput)\n","  attention_total_list_qm_all.append(attention_required_qm_divall)\n","  attention_total_list_qm_out.append(attention_required_qm_divoutput)\n","\n","end_time = time.time()\n","used_time = end_time - start_time\n","print(f\"Time: {used_time}\")\n","\n","# save the data into the drive\n","save_data = {\"attention_scores_a1\": attention_total_list_a1, \"attention_scores_a1_all\": attention_total_list_a1_all,\n","             \"attention_scores_a1_out\": attention_total_list_a1_out, \"attention_scores_qm_all\":attention_total_list_qm_all, \"attention_scores_qm_out\":attention_total_list_qm_out}\n","torch.save(save_data, data_path + f\"attention_research_data/attention_data_divided_lasttry_9000.pt\") #store the result to google drive\n","\n","\n"],"metadata":{"id":"jyatdzjBd7ew","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0638d623-0a94-4efb-81b5-cd905a2c09e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3000/3000 [1:20:57<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Time: 4857.920122146606\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ikvz8_hMjlnJ"},"source":["### Classifer training_MLP\n"]},{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","drive.mount('/content/drive', force_remount=True)\n","data_path = '/content/drive/My Drive/MSBD5002_project/' # modify this line for your drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HglDuLkYQHV3","outputId":"e5b96e47-788b-4683-b7a8-ee96de8e90db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV-cJNrhjyhP"},"outputs":[],"source":["import torch.nn as nn\n","from torch.optim import Adam\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","from tqdm import tqdm\n","import os"]},{"cell_type":"code","source":["# Load the artifact from the drive\n","training_data = {\"attention_scores_a1\": torch.tensor([]), \"attention_scores_a1_all\": torch.tensor([]),\n","             \"attention_scores_a1_out\": torch.tensor([]), \"attention_scores_qm_all\":torch.tensor([]), \"attention_scores_qm_out\":torch.tensor([])}\n","for i in range(3):\n","  data_load = torch.load(data_path + f\"attention_research_data/attention_data_divided_lasttry_{3000*(i+1)}.pt\") # it may takes around 1 minute\n","  if i == 0:\n","    training_data[\"attention_scores_a1\"] = data_load[\"attention_scores_a1\"]\n","    training_data[\"attention_scores_a1_all\"] = data_load[\"attention_scores_a1_all\"]\n","    training_data[\"attention_scores_a1_out\"] = data_load[\"attention_scores_a1_out\"]\n","    training_data[\"attention_scores_qm_all\"] = data_load[\"attention_scores_qm_all\"]\n","    training_data[\"attention_scores_qm_out\"] = data_load[\"attention_scores_qm_out\"]\n","  else:\n","    training_data[\"attention_scores_a1\"] = training_data[\"attention_scores_a1\"] + data_load[\"attention_scores_a1\"]\n","    training_data[\"attention_scores_a1_all\"] = training_data[\"attention_scores_a1_all\"] + data_load[\"attention_scores_a1_all\"]\n","    training_data[\"attention_scores_a1_out\"] = training_data[\"attention_scores_a1_out\"] + data_load[\"attention_scores_a1_out\"]\n","    training_data[\"attention_scores_qm_all\"] = training_data[\"attention_scores_qm_all\"] + data_load[\"attention_scores_qm_all\"]\n","    training_data[\"attention_scores_qm_out\"] = training_data[\"attention_scores_qm_out\"] + data_load[\"attention_scores_qm_out\"]\n","\n","labels = []\n","for i in range(9):\n","  data_load = torch.load(data_path + f\"attention_data_{1000*(i+1)}.pt\") # it may takes around 1 minute\n","  labels.append(data_load['labels'])\n","labels = np.array(labels).flatten()"],"metadata":{"id":"WmT_PwL5mBWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data['labels'] = labels\n","roc_labels = np.array(training_data[\"labels\"], dtype=np.float32)\n","roc_labels.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYy5xa_vpSQV","outputId":"723e6f38-c07f-4540-976f-aeb980d5336d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float32(4065.0)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"id":"0K2QYjHXjzOG","outputId":"de2d538c-6498-41c4-8d30-4dc5e9fe2e09"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["attention_scores.shape: (9000, 32)\n"]},{"output_type":"stream","name":"stderr","text":["Training:   1%|          | 10/1000 [00:06<13:59,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/1000],Train Loss: 0.6751,Test Loss: 0.6769,Test Accuracy: 0.6106,Test AUROC: 0.6385\n"]},{"output_type":"stream","name":"stderr","text":["Training:   2%|▏         | 20/1000 [00:08<05:27,  2.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [20/1000],Train Loss: 0.6718,Test Loss: 0.6725,Test Accuracy: 0.5950,Test AUROC: 0.6560\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 30/1000 [00:11<05:13,  3.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [30/1000],Train Loss: 0.6706,Test Loss: 0.6752,Test Accuracy: 0.5911,Test AUROC: 0.6525\n"]},{"output_type":"stream","name":"stderr","text":["Training:   4%|▍         | 40/1000 [00:13<05:08,  3.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [40/1000],Train Loss: 0.6677,Test Loss: 0.6730,Test Accuracy: 0.5961,Test AUROC: 0.6587\n"]},{"output_type":"stream","name":"stderr","text":["Training:   5%|▌         | 50/1000 [00:16<05:08,  3.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [50/1000],Train Loss: 0.6653,Test Loss: 0.6691,Test Accuracy: 0.6028,Test AUROC: 0.6595\n"]},{"output_type":"stream","name":"stderr","text":["Training:   6%|▌         | 60/1000 [00:18<05:04,  3.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [60/1000],Train Loss: 0.6631,Test Loss: 0.6737,Test Accuracy: 0.5989,Test AUROC: 0.6506\n"]},{"output_type":"stream","name":"stderr","text":["Training:   7%|▋         | 71/1000 [00:21<04:19,  3.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [70/1000],Train Loss: 0.6632,Test Loss: 0.6685,Test Accuracy: 0.6222,Test AUROC: 0.6564\n"]},{"output_type":"stream","name":"stderr","text":["Training:   8%|▊         | 80/1000 [00:23<04:48,  3.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [80/1000],Train Loss: 0.6582,Test Loss: 0.6691,Test Accuracy: 0.6083,Test AUROC: 0.6560\n"]},{"output_type":"stream","name":"stderr","text":["Training:   9%|▉         | 89/1000 [00:25<04:22,  3.48it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-59c710c7dcc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Training loop with evaluation every 10 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mtrain_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import AdamW\n","from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n","from tensorflow.keras.regularizers import l2\n","from sklearn.metrics import roc_auc_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tqdm import tqdm\n","from sklearn.preprocessing import OneHotEncoder\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Prepare and regularize data\n","attention_scores = np.stack([score.numpy() for score in training_data[\"attention_scores_a1_out\"]])\n","scaler = StandardScaler()\n","attention_scores = scaler.fit_transform(attention_scores)\n","\n","\n","roc_labels = np.array(training_data[\"labels\"], dtype=np.float32)  # 0/1 for binary classification\n","labels = tf.keras.utils.to_categorical(np.array(training_data[\"labels\"], dtype=np.int32), num_classes=2) #labels with one-hot-encoding\n","print(f\"attention_scores.shape: {attention_scores.shape}\")\n","\n","\n","\n","# Random 80/20 split\n","train_indices, test_indices = train_test_split(\n","    np.arange(len(labels)), test_size=0.2, random_state=21, shuffle=True\n",")\n","\n","train_attention = attention_scores[train_indices]\n","train_labels = labels[train_indices]\n","test_attention = attention_scores[test_indices]\n","test_labels = labels[test_indices]\n","roc_labels = roc_labels[test_indices]\n","\n","# Define model\n","input_dim = attention_scores.shape[1]\n","hidden_dim = 256  # Smaller hidden layer for simpler classifier\n","\n","model = Sequential([\n","    Dense(hidden_dim, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01)),\n","    Dropout(0.5),\n","    Dense(hidden_dim, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01)),\n","    Dense(2, activation='sigmoid')  # Output dim=1 for binary classification, dim=2 for one-hot-encoding\n","])\n","\n","\n","# Training parameters\n","batch_size = 96\n","epochs = 1000\n","lr = 0.0008\n","weight_decay = 0.001\n","\n","# Compile model\n","optimizer = AdamW(learning_rate=lr, weight_decay=weight_decay)\n","model.compile(\n","    optimizer=optimizer,\n","    loss=CategoricalCrossentropy(),\n","    metrics=['accuracy']\n",")\n","\n","\n","# Training loop with evaluation every 10 epochs\n","for epoch in tqdm(range(epochs), desc=\"Training\"):\n","    history = model.fit(\n","        train_attention, train_labels,\n","        batch_size=batch_size,\n","        epochs=1,\n","        verbose=0,\n","        shuffle=True\n","    )\n","\n","    if (epoch + 1) % 10 == 0:\n","        train_loss = history.history['loss'][0]\n","        train_accuracy = history.history['accuracy'][0]\n","\n","        # Evaluate on test set\n","        test_loss, test_accuracy = model.evaluate(\n","            test_attention, test_labels, verbose=0\n","        )\n","        # save the model for every 10 epoch\n","        model.save(data_path + \"/MLP_model/\" + f\"model_epochs_{epoch}.keras\")\n","        # Calculate AUROC\n","        test_outputs = model.predict(test_attention, verbose=0)[:, 1]\n","        auroc = roc_auc_score(roc_labels, test_outputs)\n","\n","        print(f'Epoch [{epoch+1}/{epochs}],'\n","              f'Train Loss: {train_loss:.4f},'\n","              f'Test Loss: {test_loss:.4f},'\n","              f'Test Accuracy: {test_accuracy:.4f},'\n","              f'Test AUROC: {auroc:.4f}')"]},{"cell_type":"markdown","source":["### Classifer training_XGB\n","This part is independent to the MLP classifier"],"metadata":{"id":"v878Ke6ZlCwc"}},{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","drive.mount('/content/drive', force_remount=True)\n","data_path = '/content/drive/My Drive/MSBD5002_project/' # modify this line for your drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5b96e47-788b-4683-b7a8-ee96de8e90db","id":"j-GSpaDi2Z2y"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDam2i4U2cLo"},"outputs":[],"source":["import torch.nn as nn\n","from torch.optim import Adam\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","from tqdm import tqdm\n","import os"]},{"cell_type":"code","source":["# Load the artifact from the drive\n","training_data = {\"attention_scores_a1\": torch.tensor([]), \"attention_scores_a1_all\": torch.tensor([]),\n","             \"attention_scores_a1_out\": torch.tensor([]), \"attention_scores_qm_all\":torch.tensor([]), \"attention_scores_qm_out\":torch.tensor([])}\n","for i in range(3):\n","  data_load = torch.load(data_path + f\"attention_research_data/attention_data_divided_lasttry_{3000*(i+1)}.pt\") # it may takes around 1 minute\n","  if i == 0:\n","    training_data[\"attention_scores_a1\"] = data_load[\"attention_scores_a1\"]\n","    training_data[\"attention_scores_a1_all\"] = data_load[\"attention_scores_a1_all\"]\n","    training_data[\"attention_scores_a1_out\"] = data_load[\"attention_scores_a1_out\"]\n","    training_data[\"attention_scores_qm_all\"] = data_load[\"attention_scores_qm_all\"]\n","    training_data[\"attention_scores_qm_out\"] = data_load[\"attention_scores_qm_out\"]\n","  else:\n","    training_data[\"attention_scores_a1\"] = training_data[\"attention_scores_a1\"] + data_load[\"attention_scores_a1\"]\n","    training_data[\"attention_scores_a1_all\"] = training_data[\"attention_scores_a1_all\"] + data_load[\"attention_scores_a1_all\"]\n","    training_data[\"attention_scores_a1_out\"] = training_data[\"attention_scores_a1_out\"] + data_load[\"attention_scores_a1_out\"]\n","    training_data[\"attention_scores_qm_all\"] = training_data[\"attention_scores_qm_all\"] + data_load[\"attention_scores_qm_all\"]\n","    training_data[\"attention_scores_qm_out\"] = training_data[\"attention_scores_qm_out\"] + data_load[\"attention_scores_qm_out\"]\n","\n","labels = []\n","for i in range(9):\n","  data_load = torch.load(data_path + f\"attention_data_{1000*(i+1)}.pt\") # it may takes around 1 minute\n","  labels.append(data_load['labels'])\n","labels = np.array(labels).flatten()"],"metadata":{"id":"WW_fCYBq2e6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data['labels'] = labels\n","roc_labels = np.array(training_data[\"labels\"], dtype=np.float32)\n","roc_labels.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"723e6f38-c07f-4540-976f-aeb980d5336d","id":"3r6wnOaM2jV3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float32(4065.0)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUPeX07hmUAT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf816038-effe-4242-b65d-39b22c4eeb68"},"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost AUROC: 0.7140 i = 0\n","XGBoost AUROC: 0.7141 i = 1\n","XGBoost AUROC: 0.7141 i = 2\n","XGBoost AUROC: 0.7143 i = 3\n","XGBoost AUROC: 0.7142 i = 4\n","XGBoost AUROC: 0.7143 i = 5\n","XGBoost AUROC: 0.7143 i = 6\n","XGBoost AUROC: 0.7141 i = 7\n","XGBoost AUROC: 0.7141 i = 8\n","XGBoost AUROC: 0.7141 i = 9\n","XGBoost AUROC: 0.7141 i = 10\n","XGBoost AUROC: 0.7142 i = 11\n","XGBoost AUROC: 0.7141 i = 12\n","XGBoost AUROC: 0.7141 i = 13\n","XGBoost AUROC: 0.7140 i = 14\n","XGBoost AUROC: 0.7140 i = 15\n","XGBoost AUROC: 0.7139 i = 16\n","XGBoost AUROC: 0.7138 i = 17\n","XGBoost AUROC: 0.7137 i = 18\n","XGBoost AUROC: 0.7137 i = 19\n"]}],"source":["from xgboost import XGBClassifier\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Reformatting the datasets\n","attention_scores = np.stack([score.numpy() for score in training_data[\"attention_scores_a1_out\"]])\n","scaler = StandardScaler()\n","attention_scores = scaler.fit_transform(attention_scores)\n","labels = np.array(training_data[\"labels\"], dtype=np.float32)  # 0/1 for binary classification\n","\n","\n","\n","# Random 80/20 split\n","train_indices, test_indices = train_test_split(\n","    np.arange(len(labels)), test_size=0.2, random_state=21, shuffle=True\n",")\n","\n","# Split the datasets into trainset and testset\n","train_attention = attention_scores[train_indices]\n","train_labels = labels[train_indices]\n","test_attention = attention_scores[test_indices]\n","test_labels = labels[test_indices]\n","\n","# Training for the XGB model\n","\n","xgb = XGBClassifier(n_estimators=850, learning_rate=0.0081, max_depth= 13, eval_metric='auc', tree_method = \"hist\", device = \"cuda\")\n","xgb.fit(train_attention, train_labels)\n","test_outputs_xgb = xgb.predict_proba(test_attention)[:, 1]\n","auroc_xgb = roc_auc_score(test_labels, test_outputs_xgb)\n","print(f'XGBoost AUROC: {auroc_xgb:.4f}')"]},{"cell_type":"code","source":["# Save the model\n","xgb.save_model(data_path + f\"xgbmodel_attention_research_0.66.json\")"],"metadata":{"id":"6UqoPBQdqIjF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"96cb2681d1514570b7533fd0fd8a43d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63bc0b9194fd45e1a8b8ce3e31f925cd","IPY_MODEL_252b35975fbf4e21978d5147f0940c63","IPY_MODEL_0ad57e7b467f465988d0f9fd45f0bc3e"],"layout":"IPY_MODEL_a83f191f1dec48b89c3387ca769c0415"}},"63bc0b9194fd45e1a8b8ce3e31f925cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5afc4dfba6d487b99e63270fd108255","placeholder":"​","style":"IPY_MODEL_ae2df56e97bd4c238be039e411542534","value":"Loading checkpoint shards: 100%"}},"252b35975fbf4e21978d5147f0940c63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3890e60fc024b4aaaa197ff5ed6fbc0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d2ffdd63e4e4d778db2b5954de9027b","value":2}},"0ad57e7b467f465988d0f9fd45f0bc3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e93eb50ee86499582b6a595f37991c7","placeholder":"​","style":"IPY_MODEL_28569c8894424c6681cf4b6e307c4b89","value":" 2/2 [00:04&lt;00:00,  1.94s/it]"}},"a83f191f1dec48b89c3387ca769c0415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5afc4dfba6d487b99e63270fd108255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2df56e97bd4c238be039e411542534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3890e60fc024b4aaaa197ff5ed6fbc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d2ffdd63e4e4d778db2b5954de9027b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e93eb50ee86499582b6a595f37991c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28569c8894424c6681cf4b6e307c4b89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}