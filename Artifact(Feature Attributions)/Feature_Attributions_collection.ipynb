{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d43903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "import re\n",
    "from captum.attr import IntegratedGradients\n",
    "from string import Template\n",
    "\n",
    "# Dataset\n",
    "start = 0\n",
    "end = 9000\n",
    "\n",
    "# IO\n",
    "data_dir = Path(\".\") # Where our data files are stored\n",
    "model_dir = Path(\"./.cache/models/\") # Cache for huggingface models\n",
    "results_dir = Path(\"./results/\") # Directory for storing results\n",
    "\n",
    "# Hardware\n",
    "gpu = \"0\"\n",
    "device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Integrated Grads\n",
    "ig_steps = 64\n",
    "internal_batch_size = 4\n",
    "\n",
    "# Model\n",
    "model_name = \"open_llama_7b\"\n",
    "layer_number = -1\n",
    "\n",
    "model_num_layers = {\n",
    "    \"open_llama_7b\" : 32\n",
    "}\n",
    "assert layer_number < model_num_layers[model_name]\n",
    "coll_str = \"[0-9]+\" if layer_number==-1 else str(layer_number)\n",
    "model_repos = {\n",
    "    \"open_llama_7b\" : (\"openlm-research\", f\".*model.layers.{coll_str}.mlp.up_proj\", f\".*model.layers.{coll_str}.self_attn.o_proj\")\n",
    "}\n",
    "\n",
    "def get_stop_token():\n",
    "    if \"llama\" in model_name:\n",
    "        stop_token = 13\n",
    "    elif \"falcon\" in model_name:\n",
    "        stop_token = 193\n",
    "    else:\n",
    "        stop_token = 50118\n",
    "    return stop_token\n",
    "\n",
    "def load_data():\n",
    "    trivia_qa = load_dataset('trivia_qa')\n",
    "    full_dataset = []\n",
    "    for obs in tqdm(trivia_qa['train'].select(range(9000))):\n",
    "        aliases = []\n",
    "        aliases.extend(obs['answer']['aliases'])\n",
    "        aliases.extend(obs['answer']['normalized_aliases'])\n",
    "        aliases.append(obs['answer']['value'])\n",
    "        aliases.append(obs['answer']['normalized_value'])\n",
    "        full_dataset.append((obs['question'], aliases))\n",
    "    dataset = full_dataset[start: end]\n",
    "    return dataset\n",
    "\n",
    "def get_next_token(x, model):\n",
    "    with torch.no_grad():\n",
    "        return model(x).logits\n",
    "\n",
    "def generate_response(x, model, *, max_length=100, pbar=False):\n",
    "    response = []\n",
    "    bar = tqdm(range(max_length)) if pbar else range(max_length)\n",
    "    for step in bar:\n",
    "        logits = get_next_token(x, model)\n",
    "        next_token = logits.squeeze()[-1].argmax()\n",
    "        x = torch.concat([x, next_token.view(1, -1)], dim=1)\n",
    "        response.append(next_token)\n",
    "        if next_token == get_stop_token() and step>5:\n",
    "            break\n",
    "    return torch.stack(response).cpu().numpy() #, logits.squeeze()\n",
    "\n",
    "\n",
    "def answer_question(question, model, tokenizer, *, max_length=100, pbar=False):\n",
    "    input_ids = tokenizer(question, return_tensors='pt').input_ids.to(model.device)\n",
    "    response = generate_response(input_ids, model, max_length=max_length, pbar=pbar) #, logits\n",
    "    return response# , logits , input_ids.shape[-1]\n",
    "\n",
    "\n",
    "def answer_trivia(question, targets, model, tokenizer):\n",
    "    response = answer_question(question, model, tokenizer) # , start_pos , logits\n",
    "    str_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    correct = False\n",
    "    for alias in targets:\n",
    "        if alias.lower() in str_response.lower():\n",
    "            correct = True\n",
    "            break\n",
    "    return response, str_response, correct #, logits\n",
    "\n",
    "def normalize_attributes(attributes: torch.Tensor) -> torch.Tensor:\n",
    "        # attributes has shape (batch, sequence size, embedding dim)\n",
    "        attributes = attributes.squeeze(0)\n",
    "\n",
    "        # if aggregation == \"L2\":  # norm calculates a scalar value (L2 Norm)\n",
    "        norm = torch.norm(attributes, dim=1)\n",
    "        attributes = norm / torch.sum(norm)  # Normalize the values so they add up to 1\n",
    "        \n",
    "        return attributes\n",
    "\n",
    "\n",
    "def model_forward(input_: torch.Tensor, model, extra_forward_args: Dict[str, Any]) \\\n",
    "            -> torch.Tensor:\n",
    "        output = model(inputs_embeds=input_, **extra_forward_args)\n",
    "        return torch.nn.functional.softmax(output.logits[:, -1, :], dim=-1)\n",
    "\n",
    "\n",
    "def get_embedder(model):\n",
    "    if \"falcon\" in model_name:\n",
    "        return model.transformer.word_embeddings\n",
    "    elif \"opt\" in model_name:\n",
    "        return model.model.decoder.embed_tokens\n",
    "    elif \"llama\" in model_name:\n",
    "        return model.model.embed_tokens\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model {model_name}\")\n",
    "\n",
    "def get_ig(prompt, forward_func, tokenizer, embedder, model):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    prediction_id = get_next_token(input_ids, model).squeeze()[-1].argmax()\n",
    "    encoder_input_embeds = embedder(input_ids).detach()\n",
    "    ig = IntegratedGradients(forward_func=forward_func)\n",
    "    attributes = normalize_attributes(\n",
    "        ig.attribute(\n",
    "            encoder_input_embeds,\n",
    "            target=prediction_id,\n",
    "            n_steps=ig_steps,\n",
    "            internal_batch_size=internal_batch_size\n",
    "        )\n",
    "    ).detach().cpu().numpy()\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852bd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 40.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model_loader = LlamaForCausalLM if \"llama\" in model_name else AutoModelForCausalLM\n",
    "token_loader = LlamaTokenizer if \"llama\" in model_name else AutoTokenizer\n",
    "tokenizer = token_loader.from_pretrained(f'{model_repos[model_name][0]}/{model_name}')\n",
    "model = model_loader.from_pretrained(f'{model_repos[model_name][0]}/{model_name}',\n",
    "                                        cache_dir=model_dir,\n",
    "                                        device_map=device,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        trust_remote_code=True)\n",
    "forward_func = partial(model_forward, model=model, extra_forward_args={})\n",
    "embedder = get_embedder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328d0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency\n",
    "\n",
    "def get_saliency(prompt, forward_func, tokenizer, embedder, model):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    prediction_id = get_next_token(input_ids, model).squeeze()[-1].argmax()\n",
    "    encoder_input_embeds = embedder(input_ids).detach()\n",
    "    \n",
    "    saliency = Saliency(forward_func=forward_func)\n",
    "    attributes = normalize_attributes(\n",
    "        saliency.attribute(encoder_input_embeds, target=prediction_id)\n",
    "    ).detach().to(torch.float32).cpu().numpy()\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8cd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "def get_input_x_gradient(prompt, forward_func, tokenizer, embedder, model):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    prediction_id = get_next_token(input_ids, model).squeeze()[-1].argmax()\n",
    "    encoder_input_embeds = embedder(input_ids).detach()\n",
    "    \n",
    "    input_x_gradient = InputXGradient(forward_func=forward_func)\n",
    "    attributes = normalize_attributes(\n",
    "        input_x_gradient.attribute(\n",
    "            encoder_input_embeds,\n",
    "            target=prediction_id\n",
    "        )\n",
    "    ).detach().to(torch.float32).cpu().numpy()\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560a216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [00:01<00:00, 7839.73it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results\n",
    "results = defaultdict(list)\n",
    "for idx in tqdm(range(len(dataset))):\n",
    "\n",
    "    question, answers = dataset[idx]\n",
    "    response, str_response, correct = answer_trivia(question, answers, model, tokenizer)\n",
    "    attributes_first = get_ig(question, forward_func, tokenizer, embedder, model)\n",
    "    saliency = get_saliency(question, forward_func, tokenizer, embedder, model)\n",
    "    input_x_gradient = get_input_x_gradient(question, forward_func, tokenizer, embedder, model)\n",
    "    \n",
    "    results['question'].append(question)\n",
    "    results['answers'].append(answers)\n",
    "    results['response'].append(response)\n",
    "    results['str_response'].append(str_response)\n",
    "    results['correct'].append(correct)\n",
    "    results['attributes_first'].append(attributes_first)\n",
    "    results['saliency'].append(saliency)\n",
    "    results['input_x_gradient'].append(input_x_gradient)\n",
    "\n",
    "with open(results_dir/f\"NEW_{model_name}_trivia_qa_start-{start}_end-{end}_{datetime.now().month}_{datetime.now().day}.pickle\", \"wb\") as outfile:\n",
    "    outfile.write(pickle.dumps(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac383c2a",
   "metadata": {},
   "source": [
    "# Time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772ae2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]d:\\MiniConda\\envs\\hallucination\\lib\\site-packages\\captum\\attr\\_core\\saliency.py:129: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  gradient_mask = apply_gradient_requirements(inputs_tuple)\n",
      "d:\\MiniConda\\envs\\hallucination\\lib\\site-packages\\captum\\attr\\_core\\input_x_gradient.py:118: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  gradient_mask = apply_gradient_requirements(inputs_tuple)\n",
      "100%|██████████| 10/10 [02:58<00:00, 17.88s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "ig_times = []\n",
    "saliency_times = []\n",
    "input_xgrad_times = []\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    question, answers = dataset[idx]\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    attributes_first = get_ig(question, forward_func, tokenizer, embedder, model)\n",
    "    ig_times.append(time.perf_counter() - start)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    saliency = get_saliency(question, forward_func, tokenizer, embedder, model)\n",
    "    saliency_times.append(time.perf_counter() - start)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    input_x_gradient = get_input_x_gradient(question, forward_func, tokenizer, embedder, model)\n",
    "    input_xgrad_times.append(time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5291488d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(15.533107759966516)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ig_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbacfd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1813910600030795)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(saliency_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d29ac695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9240180296200902"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.18 - 15.53)/15.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc853d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1682947299908846)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(input_xgrad_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abbb21ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9246619446233098"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.17 - 15.53)/15.53"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
